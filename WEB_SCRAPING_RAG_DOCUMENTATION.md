# Sistema Avan√ßado de Web Scraping para RAG

## üìñ Vis√£o Geral

Este sistema implementa uma solu√ß√£o completa de web scraping para sites din√¢micos com capacidades de RAG (Retrieval-Augmented Generation). Ele foi especialmente projetado para extrair conte√∫do de sites com JavaScript pesado, como a documenta√ß√£o da Autodesk.

## üéØ Caracter√≠sticas Principais

### ‚ú® Capacidades de Extra√ß√£o
- **Sites Din√¢micos**: Usando SeleniumBase para lidar com JavaScript e conte√∫do din√¢mico
- **Navega√ß√£o Inteligente**: Suporte a m√∫ltiplas abas e navega√ß√£o complexa
- **Downloads Autom√°ticos**: Extra√ß√£o de PDFs, v√≠deos, documentos e outros arquivos
- **Screenshots**: Captura autom√°tica de telas para documenta√ß√£o
- **An√°lise de Conte√∫do**: Processamento inteligente de texto e metadados

### üîç Sistema RAG Integrado
- **Chunking Inteligente**: Divis√£o otimizada de texto em chunks com sobreposi√ß√£o
- **Busca Sem√¢ntica**: Sistema TF-IDF para busca de conte√∫do relevante
- **An√°lise de Legibilidade**: Avalia√ß√£o autom√°tica da qualidade do conte√∫do
- **Metadados Ricos**: Extra√ß√£o de informa√ß√µes contextuais detalhadas

### üóÑÔ∏è Persist√™ncia e An√°lise
- **Banco SQLite**: Armazenamento estruturado de todos os dados
- **Cache Inteligente**: Sistema de cache para busca r√°pida
- **Estat√≠sticas Detalhadas**: An√°lise completa dos dados extra√≠dos
- **Exporta√ß√£o**: Suporte a CSV, JSON e outros formatos

### üåê API REST Completa
- **Extra√ß√£o Ass√≠ncrona**: Processamento em background com status em tempo real
- **Busca Interativa**: Endpoints para busca sem√¢ntica
- **Gerenciamento de Tarefas**: Controle completo de tarefas de extra√ß√£o
- **Documenta√ß√£o Swagger**: Interface web para testes

## üìÇ Estrutura do Sistema

```
rag-system/backend/
‚îú‚îÄ‚îÄ web_scraper_extractor.py          # Extrator principal com SeleniumBase
‚îú‚îÄ‚îÄ web_scraping_data_manager.py      # Gerenciador de dados e busca
‚îú‚îÄ‚îÄ web_scraping_integration.py       # API REST FastAPI
‚îú‚îÄ‚îÄ web_scraping_config.py            # Configura√ß√µes centralizadas
‚îú‚îÄ‚îÄ test_web_scraping_system.py       # Sistema de testes e demonstra√ß√£o
‚îî‚îÄ‚îÄ requirements.txt                   # Depend√™ncias atualizadas
```

## üöÄ Instala√ß√£o e Configura√ß√£o

### 1. Instala√ß√£o de Depend√™ncias

```bash
# Entre no diret√≥rio backend
cd rag-system/backend

# Instale as depend√™ncias
pip install -r requirements.txt

# Instalar drivers do Selenium (autom√°tico via SeleniumBase)
seleniumbase install chromedriver
```

### 2. Configura√ß√£o do Sistema

O sistema usa configura√ß√µes centralizadas em `web_scraping_config.py`. As principais configura√ß√µes incluem:

```python
# Configura√ß√µes b√°sicas
DEFAULT_CHUNK_SIZE = 512          # Tamanho dos chunks
DEFAULT_MAX_PAGES = 50            # M√°ximo de p√°ginas por extra√ß√£o
DEFAULT_DELAY_BETWEEN_REQUESTS = 2.0  # Delay entre requisi√ß√µes

# Configura√ß√µes da API
API_HOST = "0.0.0.0"
API_PORT = 8001
```

### 3. Teste do Sistema

```bash
# Execute o script de demonstra√ß√£o
python test_web_scraping_system.py

# Escolha a op√ß√£o 1 para demonstra√ß√£o completa
```

## üíª Uso B√°sico

### Extra√ß√£o Via C√≥digo Python

```python
from web_scraper_extractor import WebScraperExtractor

# Cria extrator
extractor = WebScraperExtractor(
    base_output_dir="minha_extracao",
    chunk_size=512,
    max_pages=20
)

# Executa extra√ß√£o
results = extractor.extract_from_website(
    start_url="https://help.autodesk.com/view/ARCHDESK/2024/ENU/",
    max_depth=2,
    same_domain_only=True
)

print(f"P√°ginas processadas: {results['extraction_summary']['total_pages_processed']}")
print(f"Chunks criados: {results['extraction_summary']['total_chunks_created']}")
```

### Gerenciamento de Dados

```python
from web_scraping_data_manager import WebScrapingDataManager

# Cria gerenciador
manager = WebScrapingDataManager("web_scraping_data")

# Busca conte√∫do
results = manager.search_chunks("AutoCAD Architecture ferramentas", limit=5)

for result in results:
    print(f"Score: {result['similarity_score']:.3f}")
    print(f"Texto: {result['text'][:100]}...")
```

### API REST

```bash
# Inicia o servidor
python web_scraping_integration.py

# Ou usando uvicorn
uvicorn web_scraping_integration:app --host 0.0.0.0 --port 8001 --reload
```

## üåê Endpoints da API

### Extra√ß√£o de Conte√∫do

```http
POST /extract
Content-Type: application/json

{
  "start_url": "https://help.autodesk.com/view/ARCHDESK/2024/ENU/",
  "max_depth": 2,
  "max_pages": 20,
  "same_domain_only": true,
  "delay_between_requests": 2.0,
  "chunk_size": 512,
  "overlap": 50
}
```

**Resposta:**
```json
{
  "task_id": "webscraping_1703123456_1234",
  "status": "queued",
  "message": "Tarefa de extra√ß√£o iniciada",
  "start_url": "https://help.autodesk.com/view/ARCHDESK/2024/ENU/"
}
```

### Status da Tarefa

```http
GET /tasks/{task_id}
```

**Resposta:**
```json
{
  "status": "running",
  "progress": 45,
  "message": "Processando p√°gina 9 de 20...",
  "start_time": "2024-01-01T10:00:00",
  "results": null
}
```

### Busca de Conte√∫do

```http
POST /search
Content-Type: application/json

{
  "query": "AutoCAD Architecture tools design",
  "limit": 10,
  "min_similarity": 0.1
}
```

**Resposta:**
```json
{
  "query": "AutoCAD Architecture tools design",
  "total_results": 3,
  "results": [
    {
      "chunk_id": "page_001_chunk_0",
      "text": "AutoCAD Architecture oferece ferramentas especializadas...",
      "similarity_score": 0.856,
      "source_url": "https://help.autodesk.com/...",
      "page_title": "Ferramentas de Desenho"
    }
  ],
  "search_timestamp": "2024-01-01T10:05:00"
}
```

### Estat√≠sticas

```http
GET /statistics
```

**Resposta:**
```json
{
  "timestamp": "2024-01-01T10:00:00",
  "statistics": {
    "database_stats": {
      "total_pages": 45,
      "total_chunks": 1250,
      "total_downloads": 12,
      "total_content_length": 125000,
      "average_readability": 65.2
    },
    "top_domains": [
      {"domain": "help.autodesk.com", "count": 30},
      {"domain": "docs.autodesk.com", "count": 15}
    ]
  }
}
```

## üîß Configura√ß√µes Avan√ßadas

### Configura√ß√µes por Dom√≠nio

O sistema permite configura√ß√µes espec√≠ficas por dom√≠nio em `web_scraping_config.py`:

```python
DOMAIN_SPECIFIC_CONFIGS = {
    "help.autodesk.com": {
        "delay_between_requests": 3.0,
        "max_pages": 30,
        "max_depth": 2,
        "custom_selectors": {
            "main_content": ["main", ".content", ".help-content"],
            "download_links": ["a[href$='.pdf']", "a[href*='download']"]
        }
    }
}
```

### Seletores CSS Personalizados

```python
CONTENT_SELECTORS = {
    "main_content": [
        'main', '[role="main"]', '.main-content',
        '.documentation', '.help-content'
    ],
    "download_links": [
        'a[href$=".pdf"]', 'a[href*="download"]'
    ]
}
```

## üìä Monitoramento e An√°lise

### Estrutura do Banco de Dados

O sistema cria automaticamente as seguintes tabelas:

- **web_pages**: P√°ginas extra√≠das com metadados
- **web_chunks**: Chunks de texto para RAG
- **downloads**: Arquivos baixados
- **extracted_links**: Links encontrados
- **content_analysis**: An√°lises de conte√∫do

### Exporta√ß√£o de Dados

```python
# Via c√≥digo
manager = WebScrapingDataManager()
exported_files = manager.export_to_csv("exports")

# Via API
GET /export/csv
```

### Limpeza de Dados Antigos

```python
# Remove dados com mais de 30 dias
deleted_count = manager.cleanup_old_data(days_old=30)
```

## üéØ Casos de Uso

### 1. Documenta√ß√£o T√©cnica (Autodesk)

```python
extractor = WebScraperExtractor(
    max_pages=50,
    delay_between_requests=3.0
)

results = extractor.extract_from_website(
    "https://help.autodesk.com/view/ARCHDESK/2024/ENU/",
    max_depth=3,
    same_domain_only=True
)
```

### 2. Site de E-commerce

```python
extractor = WebScraperExtractor(
    max_pages=100,
    delay_between_requests=1.0
)

results = extractor.extract_from_website(
    "https://example-store.com/products/",
    max_depth=2,
    same_domain_only=False
)
```

### 3. Portal de Not√≠cias

```python
extractor = WebScraperExtractor(
    chunk_size=256,  # Chunks menores para not√≠cias
    max_pages=200
)

results = extractor.extract_from_website(
    "https://news-portal.com/",
    max_depth=2
)
```

## üõ†Ô∏è Personaliza√ß√£o

### Adicionando Novos Seletores

```python
# Em web_scraping_config.py
CONTENT_SELECTORS["custom_content"] = [
    '.my-custom-selector',
    '#specific-id',
    '[data-content="main"]'
]
```

### Configurando Novos Tipos de Download

```python
# Adicione extens√µes √† configura√ß√£o
config.DOWNLOADABLE_EXTENSIONS.extend(['dwg', 'dxf', 'rvt'])
```

### Processamento P√≥s-Extra√ß√£o

```python
class CustomWebScrapingDataManager(WebScrapingDataManager):
    def store_web_page(self, page_data):
        # Processamento customizado
        page_data = self.custom_preprocessing(page_data)
        return super().store_web_page(page_data)
    
    def custom_preprocessing(self, page_data):
        # Sua l√≥gica aqui
        return page_data
```

## üö® Tratamento de Erros

O sistema inclui tratamento robusto de erros:

- **Timeouts**: Retry autom√°tico com backoff exponencial
- **Rate Limiting**: Delay adaptativo baseado na resposta do servidor
- **Conex√£o**: Tentativas m√∫ltiplas com diferentes estrat√©gias
- **Parsing**: Fallbacks para diferentes formatos de conte√∫do

## üìà Performance

### Otimiza√ß√µes Implementadas

- **Cache de Chunks**: Busca r√°pida em mem√≥ria
- **√çndices de Banco**: Otimiza√ß√£o de queries
- **Lazy Loading**: Carregamento sob demanda
- **Batch Processing**: Processamento em lotes

### Monitoramento

```python
# Estat√≠sticas de performance
stats = manager.get_statistics()
print(f"Cache hits: {stats['cache_info']['chunks_cached']}")
print(f"TF-IDF initialized: {stats['cache_info']['tfidf_initialized']}")
```

## üîí Considera√ß√µes de Seguran√ßa

- **Rate Limiting**: Respeita limites dos servidores
- **User Agents**: Headers realistas para evitar bloqueios
- **Robots.txt**: Verifica√ß√£o opcional (configur√°vel)
- **Sanitiza√ß√£o**: Limpeza de nomes de arquivos e URLs

## üìù Logs e Debug

O sistema gera logs detalhados em v√°rias categorias:

- **Extra√ß√£o**: Progress e erros de scraping
- **Banco de Dados**: Opera√ß√µes SQL e performance
- **API**: Requisi√ß√µes e respostas
- **Cache**: Hit/miss ratios e opera√ß√µes

## ü§ù Integra√ß√£o com Outros Sistemas

### Com Sistema RAG Existente

```python
# Integrar chunks com ChromaDB existente
chunks = manager.search_chunks("query", limit=10)
for chunk in chunks:
    # Adicionar ao ChromaDB
    collection.add(
        documents=[chunk['text']],
        metadatas=[chunk['metadata']],
        ids=[chunk['chunk_id']]
    )
```

### Com Sistema de Processamento de Documentos

```python
# Processar downloads automaticamente
for download in manager.downloaded_files:
    if download['filename'].endswith('.pdf'):
        # Processar PDF com docling
        process_pdf(download['local_path'])
```

## üéâ Conclus√£o

Este sistema oferece uma solu√ß√£o completa para extra√ß√£o de conte√∫do web com capacidades avan√ßadas de RAG. Ele foi projetado para ser:

- **Escal√°vel**: Processa sites grandes com efici√™ncia
- **Flex√≠vel**: Configur√°vel para diferentes tipos de sites
- **Robusto**: Tratamento abrangente de erros
- **Integr√°vel**: APIs claras para integra√ß√£o
- **Mant√≠vel**: C√≥digo bem estruturado e documentado

Para suporte ou contribui√ß√µes, consulte a documenta√ß√£o da API em `/docs` quando o servidor estiver executando.

---

**Vers√£o**: 1.0.0  
**√öltima Atualiza√ß√£o**: 2024  
**Autor**: Assistant IA  
**Licen√ßa**: MIT

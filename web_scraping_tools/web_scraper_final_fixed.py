#!/usr/bin/env python3
"""
ğŸ¯ EXTRATOR WEB SIMPLIFICADO COM DETECÃ‡ÃƒO DE IDIOMA
==================================================
Sistema simplificado que:
1. Extrai conteÃºdo de sites usando requests
2. Detecta idioma automaticamente 
3. Baixa e transcreve vÃ­deos com idioma correto
4. Gera chunks RAG unificados
"""

import json
import os
import re
import time
import requests
import subprocess
import sys
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse
from datetime import datetime
import hashlib

# Imports para web scraping
try:
    from bs4 import BeautifulSoup
    import whisper
except ImportError as e:
    print(f"âŒ Erro de importaÃ§Ã£o: {e}")
    print("Execute: pip install beautifulsoup4 openai-whisper")
    sys.exit(1)

class LanguageDetector:
    """
    ğŸŒ Detector de idioma inteligente
    """
    
    @staticmethod
    def detect_page_language(url: str, html_content: str, page_title: str = "") -> str:
        """
        Detecta idioma da pÃ¡gina usando mÃºltiplas estratÃ©gias
        """
        language = "en"  # PadrÃ£o inglÃªs
        
        # 1. Detecta pelo URL
        if "/ENU/" in url or "/en/" in url.lower() or "/en-us/" in url.lower():
            language = "en"
        elif "/PTB/" in url or "/pt/" in url.lower() or "/pt-br/" in url.lower():
            language = "pt"
        elif "/ESP/" in url or "/es/" in url.lower():
            language = "es"
        elif "/FRA/" in url or "/fr/" in url.lower():
            language = "fr"
        elif "/DEU/" in url or "/de/" in url.lower():
            language = "de"
        
        # 2. Detecta pelo HTML lang attribute
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            html_tag = soup.find('html')
            if html_tag and html_tag.get('lang'):
                lang_attr = html_tag.get('lang').lower()
                if lang_attr.startswith('en'):
                    language = "en"
                elif lang_attr.startswith('pt'):
                    language = "pt"
                elif lang_attr.startswith('es'):
                    language = "es"
                elif lang_attr.startswith('fr'):
                    language = "fr"
                elif lang_attr.startswith('de'):
                    language = "de"
        except:
            pass
        
        print(f"ğŸŒ Idioma detectado: {language} (URL: {url[:50]}...)")
        return language
    
    @staticmethod
    def get_whisper_language_code(detected_lang: str) -> str:
        """
        Converte cÃ³digo de idioma para formato Whisper
        """
        lang_map = {
            "en": "en",
            "pt": "pt", 
            "es": "es",
            "fr": "fr",
            "de": "de"
        }
        return lang_map.get(detected_lang, "en")

class VideoTranscriptionExtractor:
    """
    ğŸ¤ Extrator de transcriÃ§Ã£o de vÃ­deos com detecÃ§Ã£o de idioma
    """
    
    def __init__(self, output_dir: str = "videos"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # ConfiguraÃ§Ã£o do FFmpeg
        self.ffmpeg_path = self._find_ffmpeg()
        
        # Modelo Whisper
        self.whisper_model = None
        self._load_whisper()
        
        print(f"ğŸ¥ VideoTranscriptionExtractor inicializado")
        print(f"ğŸ“ DiretÃ³rio de transcriÃ§Ãµes: {output_dir}")
    
    def _find_ffmpeg(self) -> str:
        """Encontra o executÃ¡vel do FFmpeg"""
        possible_paths = [
            "ffmpeg",
            r"C:\Users\lucas\AppData\Local\Microsoft\WinGet\Packages\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\ffmpeg-7.1.1-full_build\bin\ffmpeg.exe",
            r"C:\ffmpeg\bin\ffmpeg.exe"
        ]
        
        for path in possible_paths:
            try:
                result = subprocess.run([path, "-version"], capture_output=True, timeout=5)
                if result.returncode == 0:
                    print(f"âœ… FFmpeg encontrado: {path}")
                    return path
            except:
                continue
        
        print("âŒ FFmpeg nÃ£o encontrado!")
        return "ffmpeg"
    
    def _load_whisper(self):
        """Carrega modelo Whisper"""
        try:
            self.whisper_model = whisper.load_model("base")
            print(f"ğŸ¤ Whisper disponÃ­vel: {self.whisper_model is not None}")
        except Exception as e:
            print(f"âŒ Erro ao carregar Whisper: {e}")
            self.whisper_model = None
    
    def transcribe_video(self, video_path: str, video_id: str, language: str = "en") -> Optional[Dict[str, Any]]:
        """Transcreve vÃ­deo usando Whisper com idioma especÃ­fico"""
        if not self.whisper_model:
            print("âŒ Whisper nÃ£o disponÃ­vel")
            return None
        
        transcript_file = os.path.join(self.output_dir, f"{video_id}_transcript.json")
        
        if os.path.exists(transcript_file):
            print(f"âœ… TranscriÃ§Ã£o jÃ¡ existe: {transcript_file}")
            try:
                with open(transcript_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        try:
            print(f"ğŸ¤ Transcrevendo vÃ­deo: {video_id}")
            print(f"ğŸŒ Idioma: {language}")
            
            # Configura FFmpeg
            os.environ['FFMPEG_BINARY'] = self.ffmpeg_path
            
            # Transcreve com idioma especÃ­fico
            whisper_lang = LanguageDetector.get_whisper_language_code(language)
            result = self.whisper_model.transcribe(
                video_path, 
                language=whisper_lang,
                fp16=False
            )
            
            transcript_data = {
                "video_id": video_id,
                "text": result["text"],
                "language": result["language"],
                "detected_language": language,
                "segments": result["segments"],
                "transcribed_at": datetime.now().isoformat()
            }
            
            with open(transcript_file, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… TranscriÃ§Ã£o concluÃ­da: {len(result['segments'])} segmentos")
            print(f"ğŸ“ Texto total: {len(result['text'])} caracteres")
            
            return transcript_data
            
        except Exception as e:
            print(f"âŒ Erro na transcriÃ§Ã£o de {video_id}: {e}")
            return None

class SimpleWebScraperWithVideo:
    """
    ğŸ•·ï¸ Web scraper simplificado com extraÃ§Ã£o de vÃ­deos
    """
    
    def __init__(self, output_dir: str = "autodesk_extraction_simple"):
        self.output_dir = output_dir
        self.videos_dir = os.path.join(output_dir, "videos")
        
        os.makedirs(self.videos_dir, exist_ok=True)
        
        self.video_extractor = VideoTranscriptionExtractor(self.videos_dir)
        self.extracted_pages = []
        self.all_chunks = []
        
        # SessÃ£o HTTP
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        print(f"ğŸš€ SimpleWebScraperWithVideo inicializado")
        print(f"ğŸ“ DiretÃ³rio: {output_dir}")
    
    def get_page_content(self, url: str) -> Tuple[str, str]:
        """ObtÃ©m conteÃºdo da pÃ¡gina"""
        try:
            print(f"ğŸŒ Acessando: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.title.get_text() if soup.title else ""
            
            return response.text, title
            
        except Exception as e:
            print(f"âŒ Erro ao acessar {url}: {e}")
            return "", ""
    
    def extract_videos_from_page(self, html_content: str, page_url: str) -> List[Dict[str, Any]]:
        """Extrai informaÃ§Ãµes de vÃ­deos da pÃ¡gina"""
        videos = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # PadrÃµes de vÃ­deo Autodesk
        video_patterns = [
            {'selector': '[data-video-id]', 'attr': 'data-video-id', 'type': 'video_direct'},
            {'selector': '.vjsalt-poster[style*="background-image"]', 'attr': 'style', 'type': 'poster_detected'},
        ]
        
        for pattern in video_patterns:
            elements = soup.select(pattern['selector'])
            for element in elements:
                try:
                    if pattern['type'] == 'poster_detected':
                        style = element.get('style', '')
                        id_match = re.search(r'([a-f0-9-]{36})', style)
                        if id_match:
                            video_id = id_match.group(1)
                        else:
                            continue
                    else:
                        video_id = element.get(pattern['attr'])
                        if not video_id:
                            continue
                    
                    video_info = {
                        'video_id': video_id,
                        'source_url': page_url,
                        'detection_type': pattern['type']
                    }
                    
                    videos.append(video_info)
                    
                except Exception as e:
                    continue
        
        # Remove duplicatas
        unique_videos = []
        seen_ids = set()
        for video in videos:
            if video['video_id'] not in seen_ids:
                unique_videos.append(video)
                seen_ids.add(video['video_id'])
        
        return unique_videos
    
    def download_video(self, video_id: str) -> Optional[str]:
        """Baixa vÃ­deo usando ID"""
        video_path = os.path.join(self.videos_dir, f"{video_id}.mp4")
        
        if os.path.exists(video_path):
            print(f"âœ… VÃ­deo jÃ¡ existe: {video_path}")
            return video_path
        
        try:
            video_url = f"https://damassets.autodesk.net/content/dam/autodesk/www/videos/{video_id}.mp4"
            print(f"ğŸ“¥ Baixando vÃ­deo: {video_id}")
            
            response = self.session.get(video_url, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(video_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(video_path)
            if file_size > 1000:
                print(f"âœ… VÃ­deo baixado: {video_path} ({file_size / 1024 / 1024:.1f} MB)")
                return video_path
            else:
                os.remove(video_path)
                return None
                
        except Exception as e:
            print(f"âŒ Erro ao baixar vÃ­deo {video_id}: {e}")
            return None
    
    def process_video_complete(self, video_info: Dict[str, Any], page_language: str) -> List[Dict[str, Any]]:
        """Processa vÃ­deo completo: download + transcriÃ§Ã£o + chunks"""
        video_id = video_info['video_id']
        source_url = video_info['source_url']
        
        print(f"ğŸ¬ Processando vÃ­deo: {video_id}")
        
        # Download
        video_path = self.download_video(video_id)
        if not video_path:
            return []
        
        # TranscriÃ§Ã£o com idioma da pÃ¡gina
        transcript = self.video_extractor.transcribe_video(video_path, video_id, page_language)
        if not transcript:
            return []
        
        # Cria chunks
        chunks = []
        for i, segment in enumerate(transcript['segments']):
            chunk = {
                "id": f"{video_id}#video_segment_{i}",
                "text": segment['text'].strip(),
                "type": "video_transcript",
                "metadata": {
                    "video_id": video_id,
                    "source_url": source_url,
                    "segment_id": segment['id'],
                    "start_time": segment['start'],
                    "end_time": segment['end'],
                    "language": transcript['language'],
                    "detected_language": page_language,
                    "chunk_index": i,
                    "extracted_at": datetime.now().isoformat()
                }
            }
            chunks.append(chunk)
        
        print(f"âœ… VÃ­deo processado: {len(chunks)} chunks criados")
        return chunks
    
    def extract_text_content(self, html_content: str, url: str, title: str) -> List[Dict[str, Any]]:
        """Extrai e chunka conteÃºdo de texto"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove elementos desnecessÃ¡rios
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        text_content = soup.get_text(separator=' ', strip=True)
        text_content = re.sub(r'\s+', ' ', text_content)
        
        if len(text_content) < 100:
            return []
        
        # Chunking simples
        chunks = []
        paragraphs = text_content.split('. ')
        
        current_chunk = ""
        chunk_index = 0
        
        for paragraph in paragraphs:
            if len(current_chunk + paragraph) > 1000:
                if current_chunk:
                    chunk = {
                        "id": f"{url}#text_chunk_{chunk_index}",
                        "text": current_chunk.strip(),
                        "type": "text_content",
                        "metadata": {
                            "source_url": url,
                            "page_title": title,
                            "chunk_index": chunk_index,
                            "extracted_at": datetime.now().isoformat()
                        }
                    }
                    chunks.append(chunk)
                    chunk_index += 1
                    current_chunk = paragraph
            else:
                current_chunk += ". " + paragraph if current_chunk else paragraph
        
        if current_chunk:
            chunk = {
                "id": f"{url}#text_chunk_{chunk_index}",
                "text": current_chunk.strip(),
                "type": "text_content",
                "metadata": {
                    "source_url": url,
                    "page_title": title,
                    "chunk_index": chunk_index,
                    "extracted_at": datetime.now().isoformat()
                }
            }
            chunks.append(chunk)
        
        return chunks
    
    def extract_single_page(self, url: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Extrai uma Ãºnica pÃ¡gina"""
        try:
            print(f"ğŸ“„ Processando: {url}")
            
            html_content, title = self.get_page_content(url)
            if not html_content:
                return [], []
            
            print(f"ğŸ“Š HTML obtido: {len(html_content)} chars")
            
            # Detecta idioma
            page_language = LanguageDetector.detect_page_language(url, html_content, title)
            
            # Extrai vÃ­deos
            videos_found = self.extract_videos_from_page(html_content, url)
            video_chunks = []
            
            print(f"ğŸ¥ VÃ­deos encontrados: {len(videos_found)}")
            for video in videos_found:
                print(f"   ğŸ“¹ {video['video_id']} ({video['detection_type']})")
            
            # Processa vÃ­deos
            for video_info in videos_found:
                video_chunks.extend(
                    self.process_video_complete(video_info, page_language)
                )
            
            # Extrai texto
            text_chunks = self.extract_text_content(html_content, url, title)
            
            # Salva dados da pÃ¡gina
            page_data = {
                "url": url,
                "title": title,
                "language": page_language,
                "videos": videos_found,
                "text_chunks_count": len(text_chunks),
                "video_chunks_count": len(video_chunks),
                "extracted_at": datetime.now().isoformat()
            }
            
            self.extracted_pages.append(page_data)
            
            print(f"âœ… PÃ¡gina processada: {len(text_chunks)} texto + {len(video_chunks)} vÃ­deo chunks")
            
            return text_chunks, video_chunks
            
        except Exception as e:
            print(f"âŒ Erro ao processar {url}: {e}")
            return [], []
    
    def extract_autodesk_pages(self, urls: List[str]):
        """Extrai mÃºltiplas pÃ¡ginas do Autodesk"""
        print(f"ğŸ¯ EXTRAÃ‡ÃƒO AUTODESK COM DETECÃ‡ÃƒO DE IDIOMA")
        print("=" * 60)
        print(f"ğŸ“„ PÃ¡ginas para processar: {len(urls)}")
        
        start_time = time.time()
        
        for url in urls:
            text_chunks, video_chunks = self.extract_single_page(url)
            self.all_chunks.extend(text_chunks)
            self.all_chunks.extend(video_chunks)
        
        end_time = time.time()
        
        # Salva dados finais
        self.save_extraction_data(end_time - start_time)
        
        return {
            "pages": len(self.extracted_pages),
            "videos": sum(len(page['videos']) for page in self.extracted_pages),
            "text_chunks": len([c for c in self.all_chunks if c['type'] == 'text_content']),
            "video_chunks": len([c for c in self.all_chunks if c['type'] == 'video_transcript']),
            "total_chars": sum(len(c['text']) for c in self.all_chunks),
            "duration": end_time - start_time
        }
    
    def save_extraction_data(self, duration: float):
        """Salva dados extraÃ­dos"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Dados finais
        final_data = {
            "extraction_info": {
                "timestamp": timestamp,
                "pages": len(self.extracted_pages),
                "total_chunks": len(self.all_chunks),
                "duration": duration
            },
            "pages": self.extracted_pages,
            "chunks": self.all_chunks
        }
        
        output_file = os.path.join(self.output_dir, f"extraction_complete_{timestamp}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Dados salvos: {output_file}")

def main():
    """Teste do sistema"""
    print("ğŸ¯ TESTE SISTEMA CORRIGIDO COM DETECÃ‡ÃƒO DE IDIOMA")
    print("=" * 60)
    
    # URLs de teste (sites em inglÃªs)
    test_urls = [
        "https://help.autodesk.com/cloudhelp/2024/ENU/AutoCAD-WhatsNew/files/GUID-B7040851-266C-48CB-9682-654F3A6B8086.htm",
        "https://help.autodesk.com/cloudhelp/2024/ENU/AutoCAD-WhatsNew/files/GUID-0E96BDF7-DE27-4C35-A78B-800F535DAA84.htm",
        "https://help.autodesk.com/cloudhelp/2024/ENU/AutoCAD-Architecture/files/GUID-87DAB120-05C8-414A-A6C0-1AC6BCCFDBEF.htm"
    ]
    
    # Remove dados antigos
    import shutil
    output_dir = "test_final_fixed"
    if os.path.exists(output_dir):
        try:
            shutil.rmtree(output_dir)
            print(f"ğŸ—‘ï¸ Dados antigos removidos")
        except:
            pass
    
    # Cria scraper
    scraper = SimpleWebScraperWithVideo(output_dir)
    
    # Executa extraÃ§Ã£o
    results = scraper.extract_autodesk_pages(test_urls)
    
    print(f"\nğŸ“Š RESULTADO FINAL:")
    print(f"   ğŸ“„ PÃ¡ginas: {results['pages']}")
    print(f"   ğŸ¥ VÃ­deos: {results['videos']}")
    print(f"   ğŸ“ Chunks texto: {results['text_chunks']}")
    print(f"   ğŸ¬ Chunks vÃ­deo: {results['video_chunks']}")
    print(f"   ğŸ“Š Total chars: {results['total_chars']:,}")
    print(f"   â±ï¸ Tempo: {results['duration']:.1f}s")

if __name__ == "__main__":
    main()

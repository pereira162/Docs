"""
Sistema Avan√ßado de Extra√ß√£o Web com Transcri√ß√£o de V√≠deos
==========================================================

Este m√≥dulo implementa extra√ß√£o completa de sites com:
- Detec√ß√£o autom√°tica de v√≠deos
- Download e transcri√ß√£o usando Whisper
- Extra√ß√£o massiva de conte√∫do
- Integra√ß√£o RAG com conte√∫do de v√≠deo

Funcionalidades espec√≠ficas para Autodesk:
- Detec√ß√£o de v√≠deos com data-video-id
- Download de MP4/WebM
- Transcri√ß√£o com timestamps
- Chunks de v√≠deo integrados ao RAG
"""

import os
import time
import json
import requests
import traceback
from pathlib import Path
from urllib.parse import urljoin, urlparse
from datetime import datetime
import subprocess
import tempfile
from bs4 import BeautifulSoup
import textstat

# Configura√ß√µes espec√≠ficas para v√≠deos Autodesk
AUTODESK_VIDEO_CONFIGS = {
    'video_selectors': [
        'video[data-video-id]',
        '.vjsalt-poster[style*="background-image"]',
        'div[data-video-id]',
        '[data-video-processed]'
    ],
    'video_url_patterns': [
        r'https://help\.autodesk\.com/videos/([^/]+)/video\.(mp4|webm)',
        r'data-video-id="([^"]+)"',
        r'background-image:\s*url\(["\']?([^"\']+poster[^"\']*)["\']?\)'
    ],
    'transcript_selectors': [
        '.video-transcript',
        '.captions',
        '.subtitles',
        '.transcript-content'
    ]
}

class VideoTranscriptionExtractor:
    """Classe para transcri√ß√£o de v√≠deos usando Whisper"""
    
    def __init__(self, output_dir="video_transcripts"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Verificar se Whisper est√° dispon√≠vel
        self.whisper_available = self._check_whisper_installation()
        
        print(f"üé• VideoTranscriptionExtractor inicializado")
        print(f"üìÅ Diret√≥rio de transcri√ß√µes: {self.output_dir}")
        print(f"üé§ Whisper dispon√≠vel: {self.whisper_available}")
    
    def _check_whisper_installation(self):
        """Verifica se o Whisper est√° instalado"""
        try:
            import whisper
            return True
        except ImportError:
            print("‚ö†Ô∏è Whisper n√£o encontrado. Instalando...")
            try:
                subprocess.run(['pip', 'install', 'openai-whisper'], check=True, capture_output=True)
                import whisper
                return True
            except Exception as e:
                print(f"‚ùå Erro ao instalar Whisper: {e}")
                return False
    
    def download_video(self, video_url, video_id):
        """Download de v√≠deo"""
        try:
            print(f"üì• Baixando v√≠deo: {video_id}")
            
            # Arquivo de destino
            video_file = self.output_dir / f"{video_id}.mp4"
            
            if video_file.exists():
                print(f"‚úÖ V√≠deo j√° existe: {video_file}")
                return str(video_file)
            
            # Download
            response = requests.get(video_url, stream=True, timeout=60)
            response.raise_for_status()
            
            # Salvar v√≠deo
            with open(video_file, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"‚úÖ V√≠deo baixado: {video_file} ({video_file.stat().st_size / 1024 / 1024:.1f} MB)")
            return str(video_file)
            
        except Exception as e:
            print(f"‚ùå Erro ao baixar v√≠deo {video_id}: {e}")
            return None
    
    def transcribe_video(self, video_file_path, video_id):
        """Transcreve v√≠deo usando Whisper"""
        try:
            if not self.whisper_available:
                print(f"‚ö†Ô∏è Whisper n√£o dispon√≠vel para transcrever {video_id}")
                return None
            
            print(f"üé§ Transcrevendo v√≠deo: {video_id}")
            
            # Arquivo de transcri√ß√£o
            transcript_file = self.output_dir / f"{video_id}_transcript.json"
            
            if transcript_file.exists():
                print(f"‚úÖ Transcri√ß√£o j√° existe: {transcript_file}")
                with open(transcript_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            # Configurar FFmpeg path se necess√°rio
            import whisper
            
            # Tentar encontrar FFmpeg
            ffmpeg_paths = [
                r"C:\Users\lucas\AppData\Local\Microsoft\WinGet\Packages\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\ffmpeg-7.1.1-full_build\bin\ffmpeg.exe",
                r"C:\ffmpeg\bin\ffmpeg.exe",
                r"C:\Program Files\ffmpeg\bin\ffmpeg.exe"
            ]
            
            ffmpeg_path = None
            for path in ffmpeg_paths:
                if os.path.exists(path):
                    ffmpeg_path = path
                    break
            
            if ffmpeg_path:
                print(f"üîß Usando FFmpeg: {ffmpeg_path}")
                # Configurar temporariamente o PATH
                old_path = os.environ.get('PATH', '')
                ffmpeg_dir = str(Path(ffmpeg_path).parent)
                os.environ['PATH'] = ffmpeg_dir + os.pathsep + old_path
            
            # Carregar modelo (usar modelo pequeno para velocidade)
            model = whisper.load_model("base")
            
            # Transcrever
            result = model.transcribe(video_file_path, language='pt')
            
            # Restaurar PATH
            if ffmpeg_path:
                os.environ['PATH'] = old_path
            
            # Preparar dados da transcri√ß√£o
            transcript_data = {
                'video_id': video_id,
                'text': result['text'],
                'language': result['language'],
                'segments': [],
                'transcribed_at': datetime.now().isoformat()
            }
            
            # Processar segmentos com timestamps
            for segment in result['segments']:
                transcript_data['segments'].append({
                    'id': segment['id'],
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': segment['text'].strip(),
                    'tokens': segment.get('tokens', []),
                    'temperature': segment.get('temperature', 0),
                    'avg_logprob': segment.get('avg_logprob', 0),
                    'compression_ratio': segment.get('compression_ratio', 0),
                    'no_speech_prob': segment.get('no_speech_prob', 0)
                })
            
            # Salvar transcri√ß√£o
            with open(transcript_file, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Transcri√ß√£o conclu√≠da: {len(transcript_data['segments'])} segmentos")
            print(f"üìù Texto total: {len(transcript_data['text'])} caracteres")
            
            return transcript_data
            
        except Exception as e:
            print(f"‚ùå Erro ao transcrever v√≠deo {video_id}: {e}")
            traceback.print_exc()
            return None
    
    def process_video_complete(self, video_url, video_id, metadata=None):
        """Processo completo: download + transcri√ß√£o"""
        try:
            print(f"üé¨ Processando v√≠deo completo: {video_id}")
            
            # 1. Download do v√≠deo
            video_file = self.download_video(video_url, video_id)
            if not video_file:
                return None
            
            # 2. Transcri√ß√£o
            transcript = self.transcribe_video(video_file, video_id)
            if not transcript:
                return None
            
            # 3. Criar chunks do v√≠deo para RAG
            video_chunks = self._create_video_chunks(transcript, metadata)
            
            # 4. Resultado completo
            result = {
                'video_id': video_id,
                'video_url': video_url,
                'video_file': video_file,
                'transcript': transcript,
                'chunks': video_chunks,
                'metadata': metadata or {},
                'processed_at': datetime.now().isoformat()
            }
            
            print(f"‚úÖ V√≠deo processado: {len(video_chunks)} chunks criados")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Erro no processamento completo: {e}")
            return None
    
    def _create_video_chunks(self, transcript, metadata=None, chunk_duration=30):
        """Cria chunks do v√≠deo baseado em dura√ß√£o"""
        chunks = []
        
        if not transcript or not transcript.get('segments'):
            return chunks
        
        current_chunk = {
            'start_time': 0,
            'end_time': 0,
            'text': '',
            'segment_ids': []
        }
        
        for segment in transcript['segments']:
            # Se o chunk atual ficaria muito longo, criar novo chunk
            if current_chunk['text'] and (segment['start'] - current_chunk['start_time']) > chunk_duration:
                # Finalizar chunk atual
                chunks.append(self._finalize_video_chunk(current_chunk, transcript, metadata))
                
                # Iniciar novo chunk
                current_chunk = {
                    'start_time': segment['start'],
                    'end_time': segment['end'],
                    'text': segment['text'],
                    'segment_ids': [segment['id']]
                }
            else:
                # Adicionar ao chunk atual
                if not current_chunk['text']:
                    current_chunk['start_time'] = segment['start']
                
                current_chunk['end_time'] = segment['end']
                current_chunk['text'] += ' ' + segment['text']
                current_chunk['segment_ids'].append(segment['id'])
        
        # Finalizar √∫ltimo chunk
        if current_chunk['text']:
            chunks.append(self._finalize_video_chunk(current_chunk, transcript, metadata))
        
        return chunks
    
    def _finalize_video_chunk(self, chunk_data, transcript, metadata):
        """Finaliza um chunk de v√≠deo com metadados"""
        chunk_text = chunk_data['text'].strip()
        
        return {
            'text': chunk_text,
            'start_time': chunk_data['start_time'],
            'end_time': chunk_data['end_time'],
            'duration': chunk_data['end_time'] - chunk_data['start_time'],
            'segment_ids': chunk_data['segment_ids'],
            'word_count': len(chunk_text.split()),
            'char_count': len(chunk_text),
            'video_id': transcript['video_id'],
            'language': transcript.get('language', 'unknown'),
            'chunk_type': 'video_transcript',
            'metadata': metadata or {}
        }

class AdvancedWebScraperWithVideo:
    """Web Scraper avan√ßado com suporte a extra√ß√£o de v√≠deos"""
    
    def __init__(self, output_dir="advanced_extraction"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Subdiret√≥rios
        self.pages_dir = self.output_dir / "pages"
        self.videos_dir = self.output_dir / "videos"
        self.pages_dir.mkdir(exist_ok=True)
        self.videos_dir.mkdir(exist_ok=True)
        
        # Extrator de v√≠deo
        self.video_extractor = VideoTranscriptionExtractor(self.videos_dir)
        
        # Dados da extra√ß√£o
        self.extracted_pages = []
        self.extracted_videos = []
        self.processed_urls = set()
        self.failed_urls = set()
        
        # Configura√ß√µes
        self.delay_between_requests = 2.0
        self.chunk_size = 512
        self.chunk_overlap = 50
        
        print(f"üöÄ AdvancedWebScraperWithVideo inicializado")
        print(f"üìÅ Diret√≥rio principal: {self.output_dir}")
        print(f"üìÑ P√°ginas: {self.pages_dir}")
        print(f"üé• V√≠deos: {self.videos_dir}")
    
    def extract_autodesk_complete(self, start_url, max_depth=4, max_pages=100):
        """Extra√ß√£o completa do site Autodesk com v√≠deos"""
        print(f"üéØ EXTRA√á√ÉO COMPLETA DO SITE AUTODESK")
        print("=" * 60)
        print(f"üåê URL inicial: {start_url}")
        print(f"üìä Configura√ß√µes: depth={max_depth}, max_pages={max_pages}")
        print(f"üé• Transcri√ß√£o de v√≠deos: {self.video_extractor.whisper_available}")
        
        self.start_time = time.time()
        self.start_url = start_url
        
        try:
            # Usar SeleniumBase para sites din√¢micos
            from seleniumbase import SB
            
            with SB(headless=True) as sb:
                self._crawl_autodesk_with_videos(sb, start_url, max_depth, max_pages)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro com SeleniumBase: {e}")
            print("üîÑ Tentando com requests...")
            self._crawl_autodesk_fallback(start_url, max_depth, max_pages)
        
        # Salvar resultados
        self._save_complete_extraction()
        
        return self._get_extraction_summary()
    
    def _crawl_autodesk_with_videos(self, sb, start_url, max_depth, max_pages):
        """Crawling completo com extra√ß√£o de v√≠deos"""
        url_queue = [(start_url, 0)]
        processed_count = 0
        
        while url_queue and processed_count < max_pages:
            current_url, depth = url_queue.pop(0)
            
            if current_url in self.processed_urls or depth > max_depth:
                continue
            
            print(f"\nüìÑ Processando [{depth}]: {current_url}")
            
            try:
                # Navegar para p√°gina
                sb.open(current_url)
                sb.sleep(3)  # Aguardar carregamento
                
                # Aguardar elementos espec√≠ficos
                try:
                    sb.wait_for_element('body', timeout=10)
                except:
                    pass
                
                # Obter conte√∫do da p√°gina
                page_source = sb.get_page_source()
                page_title = sb.get_title()
                
                print(f"üìä HTML obtido: {len(page_source)} chars")
                
                # Processar p√°gina (texto + v√≠deos)
                page_data = self._process_page_with_videos(sb, current_url, page_source, page_title)
                
                if page_data:
                    self.extracted_pages.append(page_data)
                    self.processed_urls.add(current_url)
                    processed_count += 1
                    
                    print(f"‚úÖ P√°gina processada: {len(page_data.get('chunks', []))} chunks texto + {len(page_data.get('videos', []))} v√≠deos")
                    
                    # Buscar links para pr√≥ximas p√°ginas
                    if depth < max_depth:
                        new_links = self._extract_autodesk_links(sb, current_url)
                        print(f"üîó Encontrados {len(new_links)} links")
                        
                        for link in new_links[:20]:  # Limitar a 20 links por p√°gina
                            if link not in self.processed_urls:
                                url_queue.append((link, depth + 1))
                
                time.sleep(self.delay_between_requests)
                
            except Exception as e:
                print(f"‚ùå Erro ao processar {current_url}: {e}")
                self.failed_urls.add(current_url)
        
        print(f"\nüéØ Crawling conclu√≠do: {processed_count} p√°ginas processadas")
    
    def _process_page_with_videos(self, sb, url, html_content, page_title):
        """Processa p√°gina extraindo texto e v√≠deos"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # 1. Extrair conte√∫do de texto (como antes)
            text_content = self._extract_text_content(soup, url, page_title)
            
            # 2. Extrair v√≠deos da p√°gina
            videos = self._extract_videos_from_page(sb, soup, url)
            
            # 3. Combinar dados
            page_data = {
                'url': url,
                'title': page_title,
                'text_content': text_content.get('content', ''),
                'text_chunks': text_content.get('chunks', []),
                'videos': videos,
                'video_chunks': [],
                'total_chunks': len(text_content.get('chunks', [])),
                'total_videos': len(videos),
                'extracted_at': datetime.now().isoformat(),
                'metadata': text_content.get('metadata', {})
            }
            
            # 4. Processar v√≠deos encontrados
            for video in videos:
                video_result = self._process_video_complete(video)
                if video_result:
                    self.extracted_videos.append(video_result)
                    page_data['video_chunks'].extend(video_result.get('chunks', []))
            
            page_data['total_chunks'] = len(page_data['text_chunks']) + len(page_data['video_chunks'])
            
            return page_data
            
        except Exception as e:
            print(f"‚ùå Erro ao processar p√°gina {url}: {e}")
            return None
    
    def _extract_videos_from_page(self, sb, soup, page_url):
        """Extrai informa√ß√µes de v√≠deos da p√°gina"""
        videos = []
        
        try:
            # 1. Buscar elementos de v√≠deo diretos
            video_elements = soup.find_all('video', {'data-video-id': True})
            
            for video_elem in video_elements:
                video_id = video_elem.get('data-video-id')
                if video_id:
                    # Buscar sources do v√≠deo
                    sources = video_elem.find_all('source')
                    video_urls = []
                    
                    for source in sources:
                        src = source.get('src')
                        if src and ('mp4' in src or 'webm' in src):
                            video_urls.append({
                                'url': src,
                                'type': source.get('type', 'unknown')
                            })
                    
                    if video_urls:
                        videos.append({
                            'video_id': video_id,
                            'sources': video_urls,
                            'poster': video_elem.get('poster'),
                            'page_url': page_url,
                            'element_type': 'video_direct'
                        })
            
            # 2. Buscar posters de v√≠deo (que podem indicar v√≠deos)
            poster_elements = soup.find_all('div', class_='vjsalt-poster')
            
            for poster_elem in poster_elements:
                style = poster_elem.get('style', '')
                if 'background-image' in style and 'poster' in style:
                    # Extrair ID do v√≠deo da URL do poster
                    import re
                    video_id_match = re.search(r'videos/([^/]+)/poster', style)
                    if video_id_match:
                        video_id = video_id_match.group(1)
                        
                        # Construir URLs do v√≠deo baseado no padr√£o Autodesk
                        video_base_url = f"https://help.autodesk.com/videos/{video_id}"
                        
                        videos.append({
                            'video_id': video_id,
                            'sources': [
                                {'url': f"{video_base_url}/video.mp4", 'type': 'video/mp4'},
                                {'url': f"{video_base_url}/video.webm", 'type': 'video/webm'}
                            ],
                            'poster': f"{video_base_url}/poster",
                            'page_url': page_url,
                            'element_type': 'poster_detected'
                        })
            
            # 3. Tentar clicar em v√≠deos para ativar (SeleniumBase)
            try:
                video_buttons = sb.find_elements('.vjsalt-poster')
                for i, button in enumerate(video_buttons[:5]):  # M√°ximo 5 v√≠deos por p√°gina
                    try:
                        button.click()
                        sb.sleep(2)
                        
                        # Buscar elemento de v√≠deo ativado
                        active_video = sb.find_element('video[data-video-id]')
                        if active_video:
                            video_id = active_video.get_attribute('data-video-id')
                            
                            # Verificar se j√° foi encontrado
                            if not any(v['video_id'] == video_id for v in videos):
                                sources = sb.find_elements('video[data-video-id] source')
                                video_urls = []
                                
                                for source in sources:
                                    src = source.get_attribute('src')
                                    type_attr = source.get_attribute('type')
                                    if src:
                                        video_urls.append({'url': src, 'type': type_attr})
                                
                                if video_urls:
                                    videos.append({
                                        'video_id': video_id,
                                        'sources': video_urls,
                                        'poster': active_video.get_attribute('poster'),
                                        'page_url': page_url,
                                        'element_type': 'clicked_activated'
                                    })
                    except:
                        continue
            except:
                pass
            
            print(f"üé• V√≠deos encontrados na p√°gina: {len(videos)}")
            for video in videos:
                print(f"   üìπ {video['video_id']} ({video['element_type']})")
            
            return videos
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair v√≠deos: {e}")
            return []
    
    def _process_video_complete(self, video_info):
        """Processa um v√≠deo completamente (download + transcri√ß√£o)"""
        try:
            video_id = video_info['video_id']
            
            # Escolher melhor source (preferir MP4)
            best_source = None
            for source in video_info['sources']:
                if 'mp4' in source['type'] or 'mp4' in source['url']:
                    best_source = source
                    break
            
            if not best_source and video_info['sources']:
                best_source = video_info['sources'][0]
            
            if not best_source:
                print(f"‚ö†Ô∏è Nenhuma fonte de v√≠deo v√°lida para {video_id}")
                return None
            
            # Metadata do v√≠deo
            metadata = {
                'page_url': video_info['page_url'],
                'poster_url': video_info.get('poster'),
                'element_type': video_info['element_type'],
                'source_type': best_source['type']
            }
            
            # Processar v√≠deo completo
            result = self.video_extractor.process_video_complete(
                best_source['url'], 
                video_id, 
                metadata
            )
            
            return result
            
        except Exception as e:
            print(f"‚ùå Erro ao processar v√≠deo: {e}")
            return None
    
    def _extract_text_content(self, soup, url, page_title):
        """Extrai conte√∫do de texto da p√°gina"""
        try:
            # Configura√ß√µes para Autodesk
            content_selectors = [
                '.help-content',
                '.article-content',
                '.content-main',
                'main',
                '.documentation',
                '.video-description',
                '.transcript-content'
            ]
            
            content_parts = []
            
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    # Remover scripts e estilos
                    for script in element(["script", "style", "nav", "header", "footer"]):
                        script.decompose()
                    
                    text = element.get_text(separator=' ', strip=True)
                    if text and len(text) > 50:
                        content_parts.append(text)
            
            # Se n√£o encontrou com seletores, usar body
            if not content_parts:
                body = soup.find('body')
                if body:
                    for script in body(["script", "style", "nav", "header", "footer"]):
                        script.decompose()
                    content_parts.append(body.get_text(separator=' ', strip=True))
            
            content = '\n\n'.join(content_parts)
            
            # Criar chunks
            chunks = self._create_text_chunks(content)
            
            # Metadata
            metadata = {
                'url': url,
                'title': page_title,
                'domain': urlparse(url).netloc,
                'content_length': len(content),
                'readability_score': textstat.flesch_reading_ease(content) if content else 0
            }
            
            return {
                'content': content,
                'chunks': chunks,
                'metadata': metadata
            }
            
        except Exception as e:
            print(f"‚ùå Erro ao extrair texto: {e}")
            return {'content': '', 'chunks': [], 'metadata': {}}
    
    def _create_text_chunks(self, content):
        """Cria chunks de texto"""
        if not content:
            return []
        
        chunks = []
        words = content.split()
        words_per_chunk = self.chunk_size // 4
        overlap_words = self.chunk_overlap // 4
        
        for i in range(0, len(words), words_per_chunk - overlap_words):
            chunk_words = words[i:i + words_per_chunk]
            chunk_text = ' '.join(chunk_words)
            
            if len(chunk_text.strip()) > 50:
                chunks.append({
                    'text': chunk_text,
                    'start_index': i,
                    'word_count': len(chunk_words),
                    'char_count': len(chunk_text),
                    'chunk_type': 'text_content'
                })
        
        return chunks
    
    def _extract_autodesk_links(self, sb, base_url):
        """Extrai links espec√≠ficos da Autodesk"""
        links = set()
        
        try:
            # Seletores espec√≠ficos para Autodesk
            link_selectors = [
                'a[href*="/view/"]',
                'a[href*="/help/"]',
                'a[href*="/cloudhelp/"]',
                '.nav-link',
                '.toc-link',
                '.related-link'
            ]
            
            for selector in link_selectors:
                try:
                    elements = sb.find_elements(selector)
                    for element in elements[:30]:  # Limitar por seletor
                        href = element.get_attribute('href')
                        if href:
                            full_url = urljoin(base_url, href)
                            if self._is_valid_autodesk_url(full_url, base_url):
                                links.add(full_url)
                except:
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair links: {e}")
        
        return list(links)
    
    def _is_valid_autodesk_url(self, url, base_url):
        """Valida URLs espec√≠ficas da Autodesk"""
        try:
            parsed_url = urlparse(url)
            parsed_base = urlparse(base_url)
            
            # Mesmo dom√≠nio
            if parsed_url.netloc != parsed_base.netloc:
                return False
            
            # URLs v√°lidas da Autodesk
            valid_patterns = [
                '/view/',
                '/help/',
                '/cloudhelp/',
                '/ENU/',
                'ARCHDESK',
                'AutoCAD'
            ]
            
            if not any(pattern in url for pattern in valid_patterns):
                return False
            
            # Evitar arquivos n√£o √∫teis
            excluded_extensions = ['.jpg', '.png', '.gif', '.css', '.js', '.xml']
            if any(url.lower().endswith(ext) for ext in excluded_extensions):
                return False
            
            return True
            
        except:
            return False
    
    def _crawl_autodesk_fallback(self, start_url, max_depth, max_pages):
        """Fallback usando requests (sem v√≠deos)"""
        print("üîÑ Usando fallback requests (sem extra√ß√£o de v√≠deos)")
        
        # Implementa√ß√£o b√°sica similar ao extrator anterior
        # mas sem processamento de v√≠deos
        pass
    
    def _save_complete_extraction(self):
        """Salva todos os dados extra√≠dos"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. Dados completos das p√°ginas
        pages_file = self.output_dir / f"pages_complete_{timestamp}.json"
        with open(pages_file, 'w', encoding='utf-8') as f:
            json.dump({
                'extraction_info': {
                    'start_url': self.start_url,
                    'timestamp': timestamp,
                    'pages_processed': len(self.extracted_pages),
                    'videos_processed': len(self.extracted_videos),
                    'total_duration': time.time() - self.start_time
                },
                'pages': self.extracted_pages,
                'failed_urls': list(self.failed_urls)
            }, f, ensure_ascii=False, indent=2)
        
        # 2. Dados dos v√≠deos
        videos_file = self.output_dir / f"videos_complete_{timestamp}.json"
        with open(videos_file, 'w', encoding='utf-8') as f:
            json.dump({
                'extraction_info': {
                    'total_videos': len(self.extracted_videos),
                    'timestamp': timestamp
                },
                'videos': self.extracted_videos
            }, f, ensure_ascii=False, indent=2)
        
        # 3. Chunks unificados para RAG (texto + v√≠deo)
        chunks_file = self.output_dir / f"all_chunks_rag_{timestamp}.json"
        all_chunks = []
        
        # Chunks de texto
        for page in self.extracted_pages:
            for i, chunk in enumerate(page.get('text_chunks', [])):
                all_chunks.append({
                    'id': f"{page['url']}#text_chunk_{i}",
                    'text': chunk['text'],
                    'type': 'text_content',
                    'metadata': {
                        'source_url': page['url'],
                        'page_title': page['title'],
                        'chunk_index': i,
                        'chunk_type': 'text',
                        'extracted_at': page['extracted_at'],
                        **page.get('metadata', {})
                    }
                })
            
            # Chunks de v√≠deo
            for i, chunk in enumerate(page.get('video_chunks', [])):
                all_chunks.append({
                    'id': f"{chunk['video_id']}#video_chunk_{i}",
                    'text': chunk['text'],
                    'type': 'video_transcript',
                    'metadata': {
                        'source_url': page['url'],
                        'page_title': page['title'],
                        'video_id': chunk['video_id'],
                        'start_time': chunk['start_time'],
                        'end_time': chunk['end_time'],
                        'duration': chunk['duration'],
                        'chunk_type': 'video_transcript',
                        'language': chunk['language'],
                        **chunk.get('metadata', {})
                    }
                })
        
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(all_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ DADOS SALVOS:")
        print(f"   üìÑ P√°ginas completas: {pages_file}")
        print(f"   üé• V√≠deos completos: {videos_file}")
        print(f"   üî• Chunks RAG unificados: {chunks_file}")
    
    def _get_extraction_summary(self):
        """Resumo da extra√ß√£o completa"""
        end_time = time.time()
        duration = end_time - self.start_time
        
        total_text_chunks = sum(len(page.get('text_chunks', [])) for page in self.extracted_pages)
        total_video_chunks = sum(len(page.get('video_chunks', [])) for page in self.extracted_pages)
        total_chars = sum(page.get('text_content', 0) if isinstance(page.get('text_content'), int) else len(page.get('text_content', '')) for page in self.extracted_pages)
        total_video_duration = sum(video.get('transcript', {}).get('segments', [])[-1].get('end', 0) if video.get('transcript', {}).get('segments') else 0 for video in self.extracted_videos)
        
        return {
            'start_url': self.start_url,
            'duration_seconds': round(duration, 2),
            'pages_processed': len(self.extracted_pages),
            'videos_processed': len(self.extracted_videos),
            'total_text_chunks': total_text_chunks,
            'total_video_chunks': total_video_chunks,
            'total_chunks': total_text_chunks + total_video_chunks,
            'total_characters': total_chars,
            'total_video_duration_seconds': round(total_video_duration, 2),
            'pages_failed': len(self.failed_urls),
            'whisper_available': self.video_extractor.whisper_available
        }

def test_complete_extractor():
    """Teste do extrator completo"""
    print("üéØ TESTE DO EXTRATOR COMPLETO COM V√çDEOS")
    print("=" * 60)
    
    extractor = AdvancedWebScraperWithVideo("test_complete_autodesk")
    
    # URL da Autodesk
    autodesk_url = "https://help.autodesk.com/view/ARCHDESK/2024/ENU/"
    
    result = extractor.extract_autodesk_complete(
        autodesk_url,
        max_depth=2,
        max_pages=15  # Teste com menos p√°ginas primeiro
    )
    
    print(f"\nüìä RESULTADO COMPLETO:")
    print(f"   üìÑ P√°ginas: {result['pages_processed']}")
    print(f"   üé• V√≠deos: {result['videos_processed']}")
    print(f"   üî• Chunks texto: {result['total_text_chunks']}")
    print(f"   üé¨ Chunks v√≠deo: {result['total_video_chunks']}")
    print(f"   üìä Total chars: {result['total_characters']:,}")
    print(f"   ‚è±Ô∏è Dura√ß√£o v√≠deos: {result['total_video_duration_seconds']:.1f}s")
    print(f"   üïí Tempo total: {result['duration_seconds']:.1f}s")
    
    return result

if __name__ == "__main__":
    test_complete_extractor()

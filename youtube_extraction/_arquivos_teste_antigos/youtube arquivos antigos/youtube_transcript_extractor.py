"""
Sistema de Extra√ß√£o de Transcri√ß√µes do YouTube para RAG
Sistema abrangente para extra√ß√£o e an√°lise de transcri√ß√µes do YouTube
"""

import json
import csv
import pickle
import sqlite3
import re
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Union
from urllib.parse import urlparse, parse_qs
import hashlib

# Bibliotecas principais para YouTube
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    from youtube_transcript_api.formatters import JSONFormatter, TextFormatter
    YOUTUBE_TRANSCRIPT_AVAILABLE = True
except ImportError:
    print("WARNING: youtube-transcript-api n√£o instalado. Execute: pip install youtube-transcript-api")
    YOUTUBE_TRANSCRIPT_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    print("WARNING: requests n√£o instalado. Execute: pip install requests")
    REQUESTS_AVAILABLE = False

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class YouTubeTranscriptExtractor:
    """
    Extrator abrangente de transcri√ß√µes do YouTube para sistema RAG
    """
    
    def __init__(self, output_dir: str = "youtube_extracted_data"):
        """
        Inicializa o extrator de transcri√ß√µes do YouTube
        
        Args:
            output_dir: Diret√≥rio para salvar dados extra√≠dos
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Criar subdiret√≥rios
        self.dirs = {
            'transcripts': self.output_dir / 'transcripts',
            'metadata': self.output_dir / 'metadata',
            'chunks': self.output_dir / 'chunks',
            'rag_content': self.output_dir / 'rag_content',
            'database': self.output_dir / 'database'
        }
        
        for directory in self.dirs.values():
            directory.mkdir(exist_ok=True)
        
        # Inicializar APIs
        if YOUTUBE_TRANSCRIPT_AVAILABLE:
            self.youtube_api = YouTubeTranscriptApi()
            self.json_formatter = JSONFormatter()
            self.text_formatter = TextFormatter()
        else:
            self.youtube_api = None
            logger.warning("YouTube Transcript API n√£o dispon√≠vel")
    
    def extract_video_id(self, url: str) -> Optional[str]:
        """
        Extrai o ID do v√≠deo de uma URL do YouTube
        
        Args:
            url: URL do YouTube
            
        Returns:
            ID do v√≠deo ou None se inv√°lido
        """
        try:
            # Diferentes formatos de URL do YouTube
            patterns = [
                r'(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/)([^&\n?#]+)',
                r'youtube\.com/v/([^&\n?#]+)',
                r'youtube\.com/watch\?.*v=([^&\n?#]+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    return match.group(1)
            
            # Se j√° for um ID (sem protocolo)
            if re.match(r'^[a-zA-Z0-9_-]{11}$', url):
                return url
                
            return None
            
        except Exception as e:
            logger.error(f"Erro ao extrair ID do v√≠deo: {e}")
            return None
    
    def get_video_metadata(self, video_id: str) -> Dict[str, Any]:
        """
        Obt√©m metadados do v√≠deo usando YouTube Data API (b√°sico)
        
        Args:
            video_id: ID do v√≠deo
            
        Returns:
            Dicion√°rio com metadados b√°sicos
        """
        try:
            # Metadados b√°sicos - pode ser expandido com YouTube Data API
            metadata = {
                'video_id': video_id,
                'url': f'https://www.youtube.com/watch?v={video_id}',
                'extraction_date': datetime.now().isoformat(),
                'extractor_version': '1.0.0'
            }
            
            # Tentar obter informa√ß√µes b√°sicas da p√°gina
            if REQUESTS_AVAILABLE:
                try:
                    url = f'https://www.youtube.com/watch?v={video_id}'
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                    response = requests.get(url, headers=headers, timeout=10)
                    
                    # Extrair t√≠tulo b√°sico
                    title_match = re.search(r'<title>(.+?) - YouTube</title>', response.text)
                    if title_match:
                        metadata['title'] = title_match.group(1).strip()
                    
                    # Extrair descri√ß√£o b√°sica
                    desc_match = re.search(r'"description":{"simpleText":"([^"]+)"', response.text)
                    if desc_match:
                        metadata['description'] = desc_match.group(1)[:500]  # Primeiros 500 chars
                        
                except Exception as e:
                    logger.warning(f"N√£o foi poss√≠vel obter metadados da p√°gina: {e}")
            
            return metadata
            
        except Exception as e:
            logger.error(f"Erro ao obter metadados do v√≠deo: {e}")
            return {
                'video_id': video_id,
                'url': f'https://www.youtube.com/watch?v={video_id}',
                'extraction_date': datetime.now().isoformat(),
                'error': str(e)
            }
    
    def get_available_transcripts(self, video_id: str) -> Dict[str, Any]:
        """
        Lista todas as transcri√ß√µes dispon√≠veis para um v√≠deo
        
        Args:
            video_id: ID do v√≠deo
            
        Returns:
            Informa√ß√µes sobre transcri√ß√µes dispon√≠veis
        """
        if not self.youtube_api:
            return {'error': 'YouTube Transcript API n√£o dispon√≠vel'}
        
        try:
            transcript_list = self.youtube_api.list(video_id)
            
            available_transcripts = []
            for transcript in transcript_list:
                transcript_info = {
                    'language': transcript.language,
                    'language_code': transcript.language_code,
                    'is_generated': transcript.is_generated,
                    'is_translatable': transcript.is_translatable,
                    'translation_languages': [lang.language_code for lang in transcript.translation_languages] if transcript.is_translatable else []
                }
                available_transcripts.append(transcript_info)
            
            return {
                'video_id': video_id,
                'total_transcripts': len(available_transcripts),
                'transcripts': available_transcripts,
                'has_manual': any(not t['is_generated'] for t in available_transcripts),
                'has_generated': any(t['is_generated'] for t in available_transcripts)
            }
            
        except Exception as e:
            logger.error(f"Erro ao listar transcri√ß√µes: {e}")
            return {'error': str(e), 'video_id': video_id}
    
    def extract_transcript(self, video_id: str, languages: List[str] = None, prefer_manual: bool = True) -> Dict[str, Any]:
        """
        Extrai transcri√ß√£o de um v√≠deo
        
        Args:
            video_id: ID do v√≠deo
            languages: Lista de idiomas preferidos (ex: ['pt', 'en'])
            prefer_manual: Preferir transcri√ß√µes manuais
            
        Returns:
            Dados da transcri√ß√£o extra√≠da
        """
        if not self.youtube_api:
            return {'error': 'YouTube Transcript API n√£o dispon√≠vel'}
        
        try:
            # Definir idiomas padr√£o se n√£o especificado
            if languages is None:
                languages = ['pt', 'pt-BR', 'en', 'es']
            
            transcript_list = self.youtube_api.list(video_id)
            
            # Tentar encontrar transcri√ß√£o preferida
            transcript = None
            transcript_info = None
            
            try:
                if prefer_manual:
                    # Tentar manual primeiro
                    try:
                        transcript = transcript_list.find_manually_created_transcript(languages)
                        transcript_info = {'type': 'manual', 'language': transcript.language_code}
                    except:
                        # Se n√£o encontrar manual, tentar gerada
                        transcript = transcript_list.find_generated_transcript(languages)
                        transcript_info = {'type': 'generated', 'language': transcript.language_code}
                else:
                    # Tentar qualquer uma
                    transcript = transcript_list.find_transcript(languages)
                    transcript_info = {
                        'type': 'manual' if not transcript.is_generated else 'generated',
                        'language': transcript.language_code
                    }
            except:
                # √öltima tentativa: pegar qualquer transcri√ß√£o dispon√≠vel
                if transcript_list:
                    transcript = list(transcript_list)[0]
                    transcript_info = {
                        'type': 'manual' if not transcript.is_generated else 'generated',
                        'language': transcript.language_code
                    }
            
            if not transcript:
                return {'error': 'Nenhuma transcri√ß√£o encontrada', 'video_id': video_id}
            
            # Obter dados da transcri√ß√£o
            transcript_data = transcript.fetch()
            
            # Processar segmentos
            segments = []
            full_text = ""
            total_duration = 0
            
            for i, segment in enumerate(transcript_data):
                segment_info = {
                    'index': i,
                    'text': segment.text.strip(),
                    'start': segment.start,
                    'duration': segment.duration,
                    'end': segment.start + segment.duration
                }
                segments.append(segment_info)
                full_text += segment.text + " "
                total_duration = max(total_duration, segment_info['end'])
            
            # Compilar resultado
            result = {
                'video_id': video_id,
                'extraction_timestamp': datetime.now().isoformat(),
                'transcript_info': transcript_info,
                'total_segments': len(segments),
                'total_duration': total_duration,
                'full_text': full_text.strip(),
                'segments': segments,
                'text_length': len(full_text.strip()),
                'avg_segment_duration': total_duration / len(segments) if segments else 0
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Erro ao extrair transcri√ß√£o: {e}")
            return {'error': str(e), 'video_id': video_id}
    
    def create_rag_chunks(self, transcript_data: Dict[str, Any], chunk_size: int = 500, overlap: int = 50) -> List[Dict[str, Any]]:
        """
        Cria chunks para RAG a partir da transcri√ß√£o
        
        Args:
            transcript_data: Dados da transcri√ß√£o
            chunk_size: Tamanho do chunk em caracteres
            overlap: Sobreposi√ß√£o entre chunks
            
        Returns:
            Lista de chunks para RAG
        """
        try:
            chunks = []
            full_text = transcript_data.get('full_text', '')
            segments = transcript_data.get('segments', [])
            video_id = transcript_data.get('video_id', 'unknown')
            
            if not full_text:
                return chunks
            
            # Dividir texto em chunks
            words = full_text.split()
            current_chunk = []
            current_length = 0
            chunk_index = 0
            
            for word in words:
                current_chunk.append(word)
                current_length += len(word) + 1  # +1 para espa√ßo
                
                if current_length >= chunk_size:
                    # Criar chunk
                    chunk_text = ' '.join(current_chunk)
                    
                    # Encontrar segmentos correspondentes
                    chunk_segments = self._find_segments_for_text(chunk_text, segments)
                    
                    chunk_info = {
                        'chunk_id': f"{video_id}_chunk_{chunk_index}",
                        'video_id': video_id,
                        'chunk_index': chunk_index,
                        'text': chunk_text,
                        'text_length': len(chunk_text),
                        'word_count': len(current_chunk),
                        'segments_included': len(chunk_segments),
                        'start_time': chunk_segments[0]['start'] if chunk_segments else 0,
                        'end_time': chunk_segments[-1]['end'] if chunk_segments else 0,
                        'chunk_metadata': {
                            'language': transcript_data.get('transcript_info', {}).get('language', 'unknown'),
                            'transcript_type': transcript_data.get('transcript_info', {}).get('type', 'unknown'),
                            'extraction_date': transcript_data.get('extraction_timestamp', ''),
                            'chunk_creation_date': datetime.now().isoformat()
                        }
                    }
                    
                    chunks.append(chunk_info)
                    chunk_index += 1
                    
                    # Preparar pr√≥ximo chunk com overlap
                    overlap_words = max(0, min(overlap // 10, len(current_chunk) // 4))  # Aproxima√ß√£o
                    current_chunk = current_chunk[-overlap_words:] if overlap_words > 0 else []
                    current_length = sum(len(word) + 1 for word in current_chunk)
            
            # Chunk final se sobrar texto
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunk_segments = self._find_segments_for_text(chunk_text, segments)
                
                chunk_info = {
                    'chunk_id': f"{video_id}_chunk_{chunk_index}",
                    'video_id': video_id,
                    'chunk_index': chunk_index,
                    'text': chunk_text,
                    'text_length': len(chunk_text),
                    'word_count': len(current_chunk),
                    'segments_included': len(chunk_segments),
                    'start_time': chunk_segments[0]['start'] if chunk_segments else 0,
                    'end_time': chunk_segments[-1]['end'] if chunk_segments else 0,
                    'chunk_metadata': {
                        'language': transcript_data.get('transcript_info', {}).get('language', 'unknown'),
                        'transcript_type': transcript_data.get('transcript_info', {}).get('type', 'unknown'),
                        'extraction_date': transcript_data.get('extraction_timestamp', ''),
                        'chunk_creation_date': datetime.now().isoformat()
                    }
                }
                
                chunks.append(chunk_info)
            
            return chunks
            
        except Exception as e:
            logger.error(f"Erro ao criar chunks RAG: {e}")
            return []
    
    def _find_segments_for_text(self, text: str, segments: List[Dict]) -> List[Dict]:
        """
        Encontra segmentos que correspondem a um texto espec√≠fico
        """
        try:
            # Simplifica√ß√£o: assumir que chunks seguem ordem temporal
            # Em implementa√ß√£o mais sofisticada, poderia usar matching de texto
            words_in_text = len(text.split())
            segments_needed = max(1, words_in_text // 10)  # Aproxima√ß√£o
            
            return segments[:segments_needed] if segments else []
            
        except:
            return []
    
    def analyze_content(self, transcript_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analisa o conte√∫do da transcri√ß√£o
        
        Args:
            transcript_data: Dados da transcri√ß√£o
            
        Returns:
            An√°lise do conte√∫do
        """
        try:
            full_text = transcript_data.get('full_text', '')
            segments = transcript_data.get('segments', [])
            
            if not full_text:
                return {}
            
            # An√°lise b√°sica de texto
            analysis = {
                'statistics': {
                    'total_characters': len(full_text),
                    'total_words': len(full_text.split()),
                    'total_sentences': len([s for s in full_text.split('.') if s.strip()]),
                    'average_words_per_segment': len(full_text.split()) / len(segments) if segments else 0,
                    'total_duration_minutes': transcript_data.get('total_duration', 0) / 60
                },
                'content_analysis': {
                    'language_detected': transcript_data.get('transcript_info', {}).get('language', 'unknown'),
                    'transcript_type': transcript_data.get('transcript_info', {}).get('type', 'unknown'),
                    'readability_score': self._calculate_readability(full_text)
                },
                'keywords': self._extract_keywords(full_text),
                'topics': self._identify_topics(full_text),
                'sentiment': self._basic_sentiment(full_text)
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"Erro na an√°lise de conte√∫do: {e}")
            return {'error': str(e)}
    
    def _calculate_readability(self, text: str) -> float:
        """C√°lculo b√°sico de legibilidade"""
        try:
            words = text.split()
            sentences = [s for s in text.split('.') if s.strip()]
            
            if not sentences:
                return 0.0
            
            avg_words_per_sentence = len(words) / len(sentences)
            # F√≥rmula simplificada (Flesch Reading Ease adaptada)
            score = 206.835 - (1.015 * avg_words_per_sentence)
            return max(0, min(100, score))
            
        except:
            return 0.0
    
    def _extract_keywords(self, text: str, top_n: int = 10) -> List[str]:
        """Extra√ß√£o b√°sica de palavras-chave"""
        try:
            # Remover stopwords b√°sicas
            stopwords = {
                'o', 'a', 'e', '√©', 'de', 'do', 'da', 'em', 'um', 'uma', 'para', 'com', 'n√£o', 'que', 'se', 'os', 'as',
                'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about'
            }
            
            words = re.findall(r'\b\w+\b', text.lower())
            filtered_words = [w for w in words if len(w) > 3 and w not in stopwords]
            
            # Contar frequ√™ncia
            word_freq = {}
            for word in filtered_words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # Retornar top N
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            return [word for word, freq in sorted_words[:top_n]]
            
        except:
            return []
    
    def _identify_topics(self, text: str) -> List[str]:
        """Identifica√ß√£o b√°sica de t√≥picos"""
        try:
            # T√≥picos baseados em palavras-chave comuns
            topic_keywords = {
                'tecnologia': ['tecnologia', 'computer', 'software', 'digital', 'internet', 'app'],
                'educa√ß√£o': ['educa√ß√£o', 'ensino', 'aprender', 'escola', 'universidade', 'curso'],
                'sa√∫de': ['sa√∫de', 'medicina', 'doen√ßa', 'tratamento', 'm√©dico', 'hospital'],
                'neg√≥cios': ['neg√≥cio', 'empresa', 'vendas', 'marketing', 'cliente', 'mercado'],
                'ci√™ncia': ['ci√™ncia', 'pesquisa', 'estudo', 'descoberta', 'experimento', 'an√°lise']
            }
            
            text_lower = text.lower()
            identified_topics = []
            
            for topic, keywords in topic_keywords.items():
                score = sum(1 for keyword in keywords if keyword in text_lower)
                if score >= 2:  # Threshold m√≠nimo
                    identified_topics.append(topic)
            
            return identified_topics
            
        except:
            return []
    
    def _basic_sentiment(self, text: str) -> str:
        """An√°lise b√°sica de sentimento"""
        try:
            positive_words = ['bom', '√≥timo', 'excelente', 'fant√°stico', 'good', 'great', 'excellent', 'amazing']
            negative_words = ['ruim', 'p√©ssimo', 'terr√≠vel', 'horrible', 'bad', 'terrible', 'awful']
            
            text_lower = text.lower()
            positive_count = sum(1 for word in positive_words if word in text_lower)
            negative_count = sum(1 for word in negative_words if word in text_lower)
            
            if positive_count > negative_count:
                return 'positive'
            elif negative_count > positive_count:
                return 'negative'
            else:
                return 'neutral'
                
        except:
            return 'neutral'
    
    def save_data(self, video_id: str, transcript_data: Dict[str, Any], chunks: List[Dict[str, Any]], 
                  metadata: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, str]:
        """
        Salva todos os dados extra√≠dos em m√∫ltiplos formatos
        
        Args:
            video_id: ID do v√≠deo
            transcript_data: Dados da transcri√ß√£o
            chunks: Chunks para RAG
            metadata: Metadados do v√≠deo
            analysis: An√°lise do conte√∫do
            
        Returns:
            Caminhos dos arquivos salvos
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_prefix = f"{video_id}_{timestamp}"
            
            saved_files = {}
            
            # 1. Salvar transcri√ß√£o completa (JSON)
            transcript_file = self.dirs['transcripts'] / f"{file_prefix}_transcript.json"
            with open(transcript_file, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=2)
            saved_files['transcript_json'] = str(transcript_file)
            
            # 2. Salvar metadados
            metadata_file = self.dirs['metadata'] / f"{file_prefix}_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            saved_files['metadata'] = str(metadata_file)
            
            # 3. Salvar chunks para RAG
            chunks_file = self.dirs['chunks'] / f"{file_prefix}_chunks.json"
            with open(chunks_file, 'w', encoding='utf-8') as f:
                json.dump(chunks, f, ensure_ascii=False, indent=2)
            saved_files['chunks'] = str(chunks_file)
            
            # 4. Salvar an√°lise
            analysis_file = self.dirs['rag_content'] / f"{file_prefix}_analysis.json"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            saved_files['analysis'] = str(analysis_file)
            
            # 5. Salvar texto puro para RAG
            text_file = self.dirs['rag_content'] / f"{file_prefix}_text.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(transcript_data.get('full_text', ''))
            saved_files['text'] = str(text_file)
            
            # 6. Salvar chunks em CSV
            chunks_csv = self.dirs['chunks'] / f"{file_prefix}_chunks.csv"
            if chunks:
                with open(chunks_csv, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=chunks[0].keys())
                    writer.writeheader()
                    writer.writerows(chunks)
                saved_files['chunks_csv'] = str(chunks_csv)
            
            # 7. Salvar em banco SQLite
            db_file = self.dirs['database'] / 'youtube_transcripts.db'
            self._save_to_database(db_file, video_id, transcript_data, chunks, metadata, analysis)
            saved_files['database'] = str(db_file)
            
            # 8. Criar arquivo de resumo
            summary_file = self.dirs['rag_content'] / f"{file_prefix}_summary.json"
            summary = {
                'video_id': video_id,
                'extraction_date': timestamp,
                'files_created': saved_files,
                'statistics': {
                    'total_segments': transcript_data.get('total_segments', 0),
                    'total_chunks': len(chunks),
                    'text_length': transcript_data.get('text_length', 0),
                    'duration_minutes': transcript_data.get('total_duration', 0) / 60
                },
                'metadata': metadata,
                'analysis_summary': analysis
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            saved_files['summary'] = str(summary_file)
            
            return saved_files
            
        except Exception as e:
            logger.error(f"Erro ao salvar dados: {e}")
            return {'error': str(e)}
    
    def _save_to_database(self, db_file: Path, video_id: str, transcript_data: Dict, 
                         chunks: List[Dict], metadata: Dict, analysis: Dict):
        """Salva dados no banco SQLite"""
        try:
            conn = sqlite3.connect(db_file)
            cursor = conn.cursor()
            
            # Criar tabelas se n√£o existirem
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS videos (
                    video_id TEXT PRIMARY KEY,
                    title TEXT,
                    url TEXT,
                    extraction_date TEXT,
                    total_segments INTEGER,
                    total_duration REAL,
                    text_length INTEGER,
                    language TEXT,
                    transcript_type TEXT,
                    metadata_json TEXT,
                    analysis_json TEXT
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS chunks (
                    chunk_id TEXT PRIMARY KEY,
                    video_id TEXT,
                    chunk_index INTEGER,
                    text TEXT,
                    text_length INTEGER,
                    word_count INTEGER,
                    start_time REAL,
                    end_time REAL,
                    chunk_metadata_json TEXT,
                    FOREIGN KEY (video_id) REFERENCES videos (video_id)
                )
            ''')
            
            # Inserir dados do v√≠deo
            cursor.execute('''
                INSERT OR REPLACE INTO videos VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                video_id,
                metadata.get('title', ''),
                metadata.get('url', ''),
                transcript_data.get('extraction_timestamp', ''),
                transcript_data.get('total_segments', 0),
                transcript_data.get('total_duration', 0),
                transcript_data.get('text_length', 0),
                transcript_data.get('transcript_info', {}).get('language', ''),
                transcript_data.get('transcript_info', {}).get('type', ''),
                json.dumps(metadata),
                json.dumps(analysis)
            ))
            
            # Inserir chunks
            for chunk in chunks:
                cursor.execute('''
                    INSERT OR REPLACE INTO chunks VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    chunk['chunk_id'],
                    chunk['video_id'],
                    chunk['chunk_index'],
                    chunk['text'],
                    chunk['text_length'],
                    chunk['word_count'],
                    chunk['start_time'],
                    chunk['end_time'],
                    json.dumps(chunk.get('chunk_metadata', {}))
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Erro ao salvar no banco: {e}")
    
    def process_video(self, url_or_id: str, languages: List[str] = None, 
                     prefer_manual: bool = True, chunk_size: int = 500) -> Dict[str, Any]:
        """
        Processa um v√≠deo completo: extrai transcri√ß√£o, cria chunks RAG, analisa e salva
        
        Args:
            url_or_id: URL ou ID do v√≠deo
            languages: Idiomas preferidos
            prefer_manual: Preferir transcri√ß√µes manuais
            chunk_size: Tamanho dos chunks
            
        Returns:
            Resultado completo do processamento
        """
        try:
            logger.info(f"Iniciando processamento do v√≠deo: {url_or_id}")
            
            # 1. Extrair ID do v√≠deo
            video_id = self.extract_video_id(url_or_id)
            if not video_id:
                return {'error': 'ID do v√≠deo inv√°lido', 'input': url_or_id}
            
            logger.info(f"ID do v√≠deo extra√≠do: {video_id}")
            
            # 2. Obter metadados
            logger.info("Obtendo metadados do v√≠deo...")
            metadata = self.get_video_metadata(video_id)
            
            # 3. Listar transcri√ß√µes dispon√≠veis
            logger.info("Listando transcri√ß√µes dispon√≠veis...")
            available_transcripts = self.get_available_transcripts(video_id)
            
            # 4. Extrair transcri√ß√£o
            logger.info("Extraindo transcri√ß√£o...")
            transcript_data = self.extract_transcript(video_id, languages, prefer_manual)
            
            if 'error' in transcript_data:
                return {
                    'video_id': video_id,
                    'error': transcript_data['error'],
                    'available_transcripts': available_transcripts,
                    'metadata': metadata
                }
            
            # 5. Criar chunks para RAG
            logger.info("Criando chunks para RAG...")
            chunks = self.create_rag_chunks(transcript_data, chunk_size)
            
            # 6. Analisar conte√∫do
            logger.info("Analisando conte√∫do...")
            analysis = self.analyze_content(transcript_data)
            
            # 7. Salvar todos os dados
            logger.info("Salvando dados...")
            saved_files = self.save_data(video_id, transcript_data, chunks, metadata, analysis)
            
            # 8. Compilar resultado final
            result = {
                'success': True,
                'video_id': video_id,
                'metadata': metadata,
                'available_transcripts': available_transcripts,
                'transcript_info': transcript_data.get('transcript_info', {}),
                'statistics': {
                    'total_segments': transcript_data.get('total_segments', 0),
                    'total_chunks': len(chunks),
                    'text_length': transcript_data.get('text_length', 0),
                    'duration_minutes': transcript_data.get('total_duration', 0) / 60,
                    'average_chunk_size': sum(c['text_length'] for c in chunks) / len(chunks) if chunks else 0
                },
                'analysis': analysis,
                'saved_files': saved_files,
                'processing_timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"Processamento conclu√≠do com sucesso para v√≠deo {video_id}")
            return result
            
        except Exception as e:
            logger.error(f"Erro no processamento do v√≠deo: {e}")
            return {
                'error': str(e),
                'input': url_or_id,
                'processing_timestamp': datetime.now().isoformat()
            }
    
    def search_content(self, query: str, video_id: str = None) -> List[Dict[str, Any]]:
        """
        Busca conte√∫do nos chunks armazenados
        
        Args:
            query: Termo de busca
            video_id: ID espec√≠fico do v√≠deo (opcional)
            
        Returns:
            Lista de chunks relevantes
        """
        try:
            db_file = self.dirs['database'] / 'youtube_transcripts.db'
            if not db_file.exists():
                return []
            
            conn = sqlite3.connect(db_file)
            cursor = conn.cursor()
            
            # Busca no texto dos chunks
            if video_id:
                cursor.execute('''
                    SELECT chunk_id, video_id, chunk_index, text, start_time, end_time
                    FROM chunks 
                    WHERE video_id = ? AND text LIKE ?
                    ORDER BY chunk_index
                ''', (video_id, f'%{query}%'))
            else:
                cursor.execute('''
                    SELECT chunk_id, video_id, chunk_index, text, start_time, end_time
                    FROM chunks 
                    WHERE text LIKE ?
                    ORDER BY video_id, chunk_index
                ''', (f'%{query}%',))
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'chunk_id': row[0],
                    'video_id': row[1],
                    'chunk_index': row[2],
                    'text': row[3],
                    'start_time': row[4],
                    'end_time': row[5],
                    'video_url': f'https://www.youtube.com/watch?v={row[1]}&t={int(row[4])}s'
                })
            
            conn.close()
            return results
            
        except Exception as e:
            logger.error(f"Erro na busca: {e}")
            return []

# Exemplo de uso
if __name__ == "__main__":
    # Exemplo de processamento
    extractor = YouTubeTranscriptExtractor()
    
    # URL de exemplo
    test_url = "https://www.youtube.com/watch?v=ff89oHwvNsM"
    
    # Processar v√≠deo
    result = extractor.process_video(test_url, languages=['pt', 'en'])
    
    if result.get('success'):
        print(f"‚úÖ Processamento conclu√≠do!")
        print(f"üìä Estat√≠sticas:")
        print(f"   - Segmentos: {result['statistics']['total_segments']}")
        print(f"   - Chunks RAG: {result['statistics']['total_chunks']}")
        print(f"   - Dura√ß√£o: {result['statistics']['duration_minutes']:.1f} minutos")
        print(f"   - Tamanho do texto: {result['statistics']['text_length']} caracteres")
        print(f"\nüìÅ Arquivos salvos: {len(result['saved_files'])}")
        for file_type, path in result['saved_files'].items():
            print(f"   - {file_type}: {path}")
    else:
        print(f"‚ùå Erro: {result.get('error')}")
